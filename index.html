<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "Barlow";
		font-weight: 300;
		font-size: 18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	.bold_title {
		font-family: "Barlow Medium";
	}
	h1 {
		font-size: 32px;
		font-weight: 300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
	}

	a:link,
	a:visited {
		color: #1367a7;
		text-decoration: none;
	}

	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35),
			/* The third layer shadow */
			15px 15px 0 0px #fff,
			/* The fourth layer */
			15px 15px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fourth layer shadow */
			20px 20px 0 0px #fff,
			/* The fifth layer */
			20px 20px 1px 1px rgba(0, 0, 0, 0.35),
			/* The fifth layer shadow */
			25px 25px 0 0px #fff,
			/* The fifth layer */
			25px 25px 1px 1px rgba(0, 0, 0, 0.35);
		/* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35);
		/* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper {
		/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
			0px 0px 1px 1px rgba(0, 0, 0, 0.35),
			/* The top layer shadow */
			5px 5px 0 0px #fff,
			/* The second layer */
			5px 5px 1px 1px rgba(0, 0, 0, 0.35),
			/* The second layer shadow */
			10px 10px 0 0px #fff,
			/* The third layer */
			10px 10px 1px 1px rgba(0, 0, 0, 0.35);
		/* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}

	hr {
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>

<head>
	<title>Fetal Subcortical Segmentation</title>
	<meta property="og:image" content="resources/GraphicalAbstract.png" />
	<!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Subcortical Segmentation of the Fetal Brain in 3D
	Ultrasound using Deep Learning" />
	<meta property="og:description" content="Fetal Subcortical Segmentation" />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script>
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag() { dataLayer.push(arguments); }
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span class=bold_title style="font-size:36px">Subcortical Segmentation of the Fetal Brain in 3D
			Ultrasound using Deep Learning</span>
		<table align=center width=1000px>
			<table align=center width=1000px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://lindehesse.github.io">Linde S Hesse</a><sup>1</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px">Moska Aliasi<sup>2</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px">Felipe Moser<sup>1</sup></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px">the INTERGROWTH-21<sup>st</sup> Consortium</span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px">Monique C Haak<sup>2</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://weidixie.github.io/weidi-personal-webpage/">Weidi Xie</a><sup>3</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://www.ndcn.ox.ac.uk/team/mark-jenkinson">Mark Jenkinson</a><sup>4,5,6</sup></span>
						</center>
					</td>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="https://www.pmb.ox.ac.uk/person/dr-ana-namburete">Ana IL Namburete</a><sup>1</sup></span>
						</center>
					</td>
				</tr>
			</table>
			
			<br>
			
			<table align=center width=800px>
				<tr>
					<td align=center width=800px>
						<center>
							<span style="font-size:12px"><sup>1</sup><a href="https://omni.cs.ox.ac.uk/"> Ultrasound NeuroImage Analysis Group</a>, University of Oxford</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=800px>
				<tr>
					<td align=center width=800px>
						<center>
							<span style="font-size:12px"><sup>2</sup> Department of Obstetrics and Fetal Medicine, Leiden University Medical Center, The Netherlands</a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=800px>
				<tr>
					<td align=center width=800px>
						<center>
							<span style="font-size:12px"><sup>3</sup> Visual Geometry Group, Department of Engineering Science, University of Oxford, United Kingdom</a></span>
						</center>
					</td>
				</tr>
			</table>

			<table align=center width=800px>
				<tr>
					<td align=center width=800px>
						<center>
							<span style="font-size:12px"><sup>4</sup> Wellcome Centre for Integrative Neuroimaging, University of Oxford</a></span>
						</center>
					</td>
				</tr>
			</table>


			<table align=center width=800px>
				<tr>
					<td align=center width=800px>
						<center>
							<span style="font-size:12px"><sup>5</sup> Australian Institute for Machine Learning (AIML), University of Adelaide </a></span>
						</center>
					</td>
				</tr>
			</table>

			<table align=center width=800px>
				<tr>
					<td align=center width=800px>
						<center>
							<span style="font-size:12px"><sup>6</sup> South Australian Health and Medical Research Institute (SAHMRI) </a></span>
						</center>
					</td>
				</tr>
			</table>


			<br>


			<table align=center width=450px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href= "https://www.biorxiv.org/content/10.1101/2021.09.29.462430v1" target="blank">[preprint]</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href= "https://github.com/lindehesse/FetalSubcortSegm_Code" target="blank">[code]</a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href= "resources/PosterDesign_Final2.pdf" target="blank">[poster]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<hr>

		
	<center>
	
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:800px" img src="./resources/GraphicalAbstract.png" />
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
				</td>
			</tr>
		</table> 
	</center>
	<hr>

	<table align=center width=850px>
		<center>
			<h1>Abstract</h1>
		</center>
		<tr>
			<td>
				The quantification of subcortical volume development from 3D fetal ultrasound can provide important diagnostic information during pregnancy monitoring. However, manual segmentation of subcortical structures in ultrasound volumes is time-consuming and challenging due to low soft tissue contrast, speckle and shadowing artifacts. For this reason, we developed a convolutional neural network (CNN) for the automated segmentation of the choroid plexus (CP), lateral posterior ventricle horns (LPVH), cavum septum pellucidum et vergae (CSPV), and cerebellum (CB) from 3D ultrasound. As ground-truth labels are scarce and expensive to obtain, we applied few-shot learning, in which only a small number of manual annotations (n = 9) are used to train a CNN. We compared training a CNN with only a few individually annotated volumes versus many weakly labelled volumes obtained from atlas-based segmentations. This showed that segmentation performance close to intra-observer variability can be obtained with only a handful of manual annotations. Finally, the trained models were applied to a large number (n = 278) of ultrasound image volumes of a diverse, healthy population, obtaining novel US-specific growth curves of the respective structures during the second trimester of gestation. 

			</td>
		</tr>
	</table>

	<hr>

	<center>
		<center>
			<h1>Summary</h1>
		</center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
					<font size="+2">We train a multi-label CNN to predict 3D segmentations of subcortical structures using only a small number of manual annotations (N=9).</font>

					<br>

					<h2>Segmentation Pipeline:</h2> 
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
				<center>
					<img class="round" style="width:800px" img src="./resources/Methods3D_Figure.png" />
				</center>
				</td>
			</tr>
		</table>
		<br>
		<table align=center width=850px>
			<tr>
				<td>
				<center>
					<font size="+2">We compared training a CNN with N individually annotated 3D images (<i >expert labels</i>) versus training with many weakly labelled images (<i>atlas labels</i>) obtained from annotating N template images. </font> 

					<br>
					<h2>
					Atlas label generation<h2>
				</center>
				</td>
			</tr>
		</table>
		<table align=center width=600px>
			<tr>
				<td>
				<center>
					<img class="round" style="width: 600px" img src="./resources/AtlasLabelGeneration_cropped.png" />
				</center>
				</td>
			</tr>
		</table>
		<table align=center width=600px>
			<tr>
				<td>
				<center>
					<p style="margin : 0; padding-top:0;line-height: 1;"><font size="-1">Schematic overview of atlas label generation. Standard whole brain templates
				(top row) were constructed for each GW and all structures (CB, CP, CSPV and LPVH)
				were annotated in these templates. Cluster-based template construction (bottom row) was only performed for the LPVH. </font> </p>
				</center>
				</td>
			</tr>
		</table>
	</center>

	<hr>


	<center>
		<center>
			<h1>Results</h1>
		</center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						
					<font size="+2"><i>We obtained excellent segmentation performance using only 9 manual annotations for training:</p></b></i>
					</center>
				</td>
			</tr>
		</table>


		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" img src="./resources/07-6011_159days_0300_axial.gif" />
					</center>
				</td>
			</tr>
		</table>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" img src="./resources/07-6011_159days_0300_coronal.gif" />
					</center>
				</td>
			</tr>
		</table>
	</center>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" img src="./resources/07-6011_159days_0300_sagittal.gif"  />
					</center>
				</td>
			</tr>
		</table>
	</center>

	<br>
	
	<table align=center width=850px>
		<tr>
			<td width=260px>
				<center>
					
				<font size="+2"><i>We also compared segmentation performance for 3D images in their original acquisition orientation (<i>aligned</i>)  versus aligned to the same coordinate system as preprocessing step (<i>unaligned</i>):</p></b></i>
				</center>
			</td>
		</tr>
	</table>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" img src="./resources/Exp1_MetricsAfterCC-1.jpg"  />
					</center>
				</td>
			</tr>
		</table>
	</center>
	<table align=center width=600px>
		<tr>
			<td>
			<center>
				<p style="margin : 0; padding-top:0;line-height: 1;"><font size="-1">Resulting performance values after post-processing for a CNN trained with expert labels (&theta;<sup>exp</sup>) and a CNN trained with atlas labels (&theta;<sup>atl</sup>) </font> </p>
			</center>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=850px>
		<tr>
			<td width=260px>
				<center>
					
				<font size="+2"><i>We then applied our trained networks to a large group (N = 278) of healthy fetusses to obtain US-specific growth curves:</p></b></i>
				</center>
			</td>
		</tr>
	</table>

	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:600px" img src="./resources/struct_volume_NeuroImage_v2-1.png"  />
					</center>
				</td>
			</tr>
		</table>

		<table align=center width=600px>
			<tr>
				<td>
				<center>
					<p style="margin : 0; padding-top:0;line-height: 1;"><font size="-1">Estimated structural volumes for subcortical structures as a function of GA.
						Volumes were fitted with a linear or quadratic fit (black), in which the quadratic term
						was only added if it was significant for both networks (per structure). The 95% prediction
						confidence intervals where also computed and are shown with red dashed lines. For each
						structure, samples were colored based on their residual for &theta;<sup>exp</sup>, and the same colors (per
						sample) where used for &theta;<sup>atl</sup>. Relative volume predictions can be found in the paper. </font> </p>
				</center>
				</td>
			</tr>
		</table>
	</center>


	<hr>


	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center>
						<h1>Acknowledgements</h1>
					</center>
					LH acknowledges the support of the UK Engineering and Physical Sciences Research Council (EPSRC) Doctoral Training Award. FM acknowledges the support and funding from the Engineering and Physical Sciences Research Council (EPSRC) and Medical Research Council (MRC) (EP/L016052/1), as well as the support from University College Oxford and its Oxford-Radcliffe benefaction. WX is supported by the UK Engineering and Physical Sciences Research Council (EPSRC) Programme Grant Seebibyte (EP/M013774/1) and Grant Visual AI (EP/T028572/1). MJ is supported by the National Institute for Health Research (NIHR) Oxford Biomedical Research Centre (BRC), and this research was funded by the Wellcome Trust [215573/Z/19/Z]. The Wellcome Centre for Integrative Neuroimaging is supported by core funding from the Wellcome Trust [203139/Z/16/Z]. AN is grateful for support from the UK Royal Academy of Engineering under the Engineering for Development Research Fellowships scheme, and to St Hilda’s College, Oxford. We would also like to acknowledge the INTERGROWTH-21<sup>st</sup>  Consortium for collecting the data and making it available to us.

					<br>
					<br>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a
						href="http://richzhang.github.io/">Richard Zhang</a> for a <a
						href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found
					<a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

	<br>
</body>

</html>